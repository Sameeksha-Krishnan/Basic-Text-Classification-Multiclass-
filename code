import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

url = 'http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'
dataset = tf.keras.utils.get_file('stack_overflow_16k', url, untar = True, cache_dir = '.', cache_subdir = '')

dataset_dir = os.path.join(os.path.dirname(dataset), 'stack_overflow')
os.listdir(dataset_dir)

train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)
test_dir = os.path.join(dataset_dir, 'test')
os.listdir(test_dir)

sample_file = os.path.join(train_dir, 'csharp/0.txt')
with open(sample_file) as f:
    print(f.read())

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory('stack_overflow/train', batch_size = 32, validation_split = 0.2, subset = 'training', seed = 42)

for text_batch, label_batch in raw_train_ds.take(1):
    for i in range(10):
        print('Text:', text_batch.numpy()[i])
        print('Label:', label_batch.numpy()[i])

print('Label 0 corresponds to ', raw_train_ds.class_names[0])
print('Label 1 corresponds to ', raw_train_ds.class_names[1])
print('Label 2 corresponds to ', raw_train_ds.class_names[2])
print('Label 3 corresponds to ', raw_train_ds.class_names[3])

raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory('stack_overflow/train', batch_size = 32, validation_split = 0.2, subset = 'validation', seed = 42)
raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory('stack_overflow/test', batch_size = 32)

def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), ' a')

max_features = 10000
vectorize_layer = TextVectorization(standardize = custom_standardization, max_tokens = max_features, output_mode = 'int', output_sequence_length = 250)

train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)

def vectorize_text(text, label):
    text = tf.expand_dims(text, -1)
    return vectorize_layer(text), label

text_batch, label_batch = next(iter(raw_train_ds))
first_text, first_label = text_batch[0], label_batch[0]
print('Text:', first_text)
print('Label:', raw_train_ds.class_names[first_label])
print('Vectorized text:', vectorize_text(first_text, first_label))

print('3--->', vectorize_layer.get_vocabulary()[3])
print('4--->', vectorize_layer.get_vocabulary()[4])
print('5--->', vectorize_layer.get_vocabulary()[5])

train_ds = raw_train_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)

embedding_dim = 16
model = tf.keras.Sequential([layers.Embedding(max_features + 1, embedding_dim), layers.Dropout(0.2),
                             layers.GlobalAveragePooling1D(), layers.Dropout(0.2), layers.Dense(4)])
model.summary()

model.compile(loss = losses.SparseCategoricalCrossentropy(from_logits = True), optimizer = 'adam', metrics = tf.metrics.SparseCategoricalAccuracy())

epochs = 10
history = model.fit(train_ds, validation_data = val_ds, epochs = 10)

loss, accuracy = model.evaluate(test_ds)
print('Loss:', loss)
print('Accuracy:', accuracy)

history_dict = history.history
history_dict.keys()

acc = history_dict['sparse_categorical_accuracy']
loss = history_dict['loss']
val_acc = history_dict['val_sparse_categorical_accuracy']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label = 'Training accuracy')
plt.plot(epochs, val_acc, 'b', label = 'Validation accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

epochs = range(1, len(acc) + 1)
plt.plot(epochs, loss, 'bo', label = 'Training loss')
plt.plot(epochs, val_loss, 'b', label = 'Validation loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc = 'lower right')
plt.show()

export_model = tf.keras.Sequential([vectorize_layer, model, layers.Activation('softmax')])
export_model.compile(loss = losses.SparseCategoricalCrossentropy(from_logits = False), optimizer = 'adam', metrics = ['accuracy'])
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)

examples = ["idiomatic blank iterating and adding to a dict i'm running through a string, creating all substrings of size 10, and adding them to a dict. this is my code,..sequence_map = {}.for i in range(len(s)):.    sub = s[i:i+10].    if sub in sequence_map:.       sequence_map[sub] += 1.    else:.       sequence_map[sub] = 1...is there a way to do this more blankically?..also how do i do the reverse blankically, as in interating through the dict and composing a list where value is equal to something?..[k for k, v in sequence_map.items()]"]
export_model.predict(examples)
